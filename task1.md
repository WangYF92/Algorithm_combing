# 任务1 - 线性回归算法梳理

## 1、机器学习的一些概念

### 1.1 什么是机器学习？

**广义概念：**

机器学习是让计算机具有学习的能力，无需进行明确编程。 —— 亚瑟·萨缪尔，1959

**工程概念：**

计算机程序利用经验 E 学习任务 T，性能是 P，如果针对任务 T 的性能 P 随着经验 E 不断增长，则称为机器学习。 —— 汤姆·米切尔，1997

### 1.2 机器学习分类（是否在人类监督下进行训练）

**有监督学习**（Supervised Learning）：在监督学习中，用来训练算法的训练数据包含了答案，称为标签。

**无监督学习**（Unsupervised Learning）：在无监督学习中，训练数据是没有加标签的。系统在没有老师的条件下进行学习。

**半监督学习**：一些算法可以处理部分带标签的训练数据，通常是大量不带标签数据加上小部分带标签数据。这称作半监督学习。

**强化学习**：学习系统在这里被称为智能体（agent），可以对环境进行观察，选择和执行动作，获得奖励（负奖励是惩罚，见图 1-12）。然后它必须自己学习哪个是最佳方法（称为策略，policy），以得到长久的最大奖励。策略决定了智能体在给定情况下应该采取的行动。

### 1.3 泛化能力

指机器学习算法对新鲜样本的适应能力。学习的目的是学到隐含在数据背后的规律，对具有同一规律的学习集以外的数据，经过训练的网络也能给出合适的输出，该能力称为泛化能力。

### 1.4 过拟合、欠拟合（方差和偏差以及各自解决方法）

**过拟合**（overfitting）：当学习器把训练样本学得“太好”了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降。

解决方法：
* 简化模型，可以通过选择一个参数更少的模型（比如使用线性模型，而不是高阶多项式模型）、减少训练数据的属性数、或限制一下模型
* 收集更多的训练数据
* 减小训练数据的噪声（比如，修改数据错误和去除异常值）

**欠拟合**（underfitting）：这是指对训练样本的一般性质尚未学好。

解决方法：
* 选择一个更强大的模型，带有更多参数
* 用更好的特征训练学习算法（特征工程）
* 减小对模型的限制（比如，减小正则化超参数）

### 1.5 交叉验证

训练集分成互补的子集，每个模型用不同的子集训练，再用剩下的子集验证。一旦确定模型类型和超参数，最终的模型使用这些超参数和全部的训练集进行训练，用测试集得到推广误差率。

> 没有免费午餐公理
> 
> 模型是观察的简化版本。简化意味着舍弃无法进行推广的表面细节。但是，要确定舍弃什么数据、保留什么数据，必须要做假设。例如，线性模型的假设是数据基本上是线性的，实例和模型直线间的距离只是噪音，可以放心忽略。
> 
> 在一篇 1996 年的著名论文中，David Wolpert 证明，如果完全不对数据做假设，就没有理由选择一个模型而不选另一个。这称作没有免费午餐（NFL）公理。对于一些数据集，最佳模型是线性模型，而对其它数据集是神经网络。没有一个模型可以保证效果更好（如这个公理的名字所示）。确信的唯一方法就是测试所有的模型。因为这是不可能的，实际中就必须要做一些对数据合理的假设，只评估几个合理的模型。例如，对于简单任务，你可能是用不同程度的正则化评估线性模型，对于复杂问题，你可能要评估几个神经网络模型。

## 2、线性回归的原理

**线性回归**（Linear Regression）试图学得一个线性模型以尽可能准确地预测实值输出标记。

## 3、线性回归损失函数、代价函数、目标函数

**参考：**[损失函数 、代价函数、目标函数](https://www.jianshu.com/p/b5b09ce54a22)

## 4、优化方法(梯度下降法、牛顿法、拟牛顿法等)

### 4.1 梯度下降法

梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。

梯度下降方法的问题：
每一步走的距离在极值点附近非常重要，如果走的步子过大，容易在极值点附近震荡而无法收敛。

解决办法：将alpha设定为随着迭代次数而不断减小的变量，但是也不能完全减为零。

**参考：**[深入浅出--梯度下降法及其实现](https://www.jianshu.com/p/c7e642877b0e)

### 4.2 牛顿法

牛顿法是为了求解函数值为零的时候变量的取值问题的。

**优点：**
牛顿法收敛速度相比梯度下降法很快，而且由于海森矩阵的的逆在迭代中不断减小，起到逐渐缩小步长的效果。

**缺点：**
牛顿法的缺点就是计算海森矩阵的逆比较困难，消耗时间和计算资源。因此有了拟牛顿法。

### 4.3 拟牛顿法

牛顿法需要求海森矩阵，这个矩阵需要计算二阶偏导数，比较复杂。为了改良这个问题，提出了拟牛顿法。

基本idea是：不求二阶偏导数，构造出一个近似的海森矩阵。

**参考：**[机器学习入门之 — 梯度下降，牛顿法，拟牛顿法](https://www.jianshu.com/p/0f864a4c3b38)

## 5、线性回归的评估指标

### 5.1 均方误差 MSE（Mean Squared Error）

### 5.2 均方根误差 RMSE（Root Mean Squared Error）

所以为了消除量纲的影响，我们可以对MSE开方，得到的结果就第二个评价指标：均方根误差。

MSE 和 RMSE 二者是呈正相关的，MSE 值大，RMSE 值也大，所以在评价线性回归模型效果的时候，使用 RMSE 就可以了。

### 5.3 平均绝对误差 MAE（Mean Absolute Error）

### 5.4 R方值（R2_score）

**参考：**[【机器学习12】线性回归算法评价指标：MSE、RMSE、R2_score](https://www.jianshu.com/p/f7309124cacf)

## 6、sklearn参数详解

**模型定义**
> sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False,copy_X=True, n_jobs=1)
* fit_intercept：布尔类型，可选参数；设置模型是否计算截距，false表示不使用截距。
* normalize：布尔类型，可选参数，默认值为false；设置为true之前，解释变量x将在回归前进行标准化。
* copy_X：布尔类型，可选参数，默认值为true；如果为true，x将被复制；否则被重写。
* n_jobs：int类型，可选参数，默认值为1；如果设为1，将启动所有CPU。

**常用函数**
> fit(X,y, [sample_weight])  # 拟合线性模型
* X:训练数据，形状如 [n_samples,n_features]
* y:函数值，形状如 [n_samples, n_targets]
* sample_weight： 每个样本的个体权重,形状如[n_samples]

**参考：**[scikit-learn (sklearn) 官方文档中文版（sklearn版本0.21.3）](https://sklearn.apachecn.org/docs/0.21.3/)
> get_params([deep])  # 获取参数估计量
>
> set_params(**params) # 设置参数估计量

> predict(X) # 利用训练好的模型进行预测，返回预测的函数值
* X:预测数据集，形状如 (n_samples, n_features)

> score(X, y, [sample_weight]) # 返回预测的决定系数R^2
* X;训练数据，形状如 [n_samples,n_features]
* y;关于X的真实函数值，形状如 (n_samples) or (n_samples, n_outputs)
* sample_weight：样本权重
